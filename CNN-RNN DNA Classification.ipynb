{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this research project, we focusing on how to classify the DNA sequence by using deep learning models and benefit from deep learning capability of extracting significant features. Deep learning uses these features to improve the classification accuracy. Deep learning has advanced rapidly and have state-of-the-art performance in bioinformatics field. In genomic analysis, DNA sequence classification is one of the most important tasks. DNA sequence with bases [A, C, G, T] carry a class label that identifies the kind of the sequences. DNA sequence classification has many challenges the main challenge is feature selection. Even with new feature selection techniques, feature selection is a major problem in sequence classification. Using deep learning techniques, there is no need for manual feature extraction, deep learning extract features automatically and classify them. So, we need to decide which architecture will be able to produce a good results in DNA classification task by implement a model to deal with this problem and select how the data will be represented to fed in deep learning network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary library/package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Conv1D, MaxPool1D, LSTM, Conv2D, MaxPool2D, ConvLSTM2D, ConvRNN2D, TimeDistributed, Flatten\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.models import Sequential\n",
    "from tensorflow.contrib import learn\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(fname='histone/H3.txt', dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is read in the following format:\n",
    "\n",
    "data[0] = Some kind of sequence ID\n",
    "\n",
    "data[1] = DNA Sequence of A, T, C, and G\n",
    "\n",
    "data[2] = Binary Label (0 or 1)\n",
    "\n",
    "We only require the sequence, and the labels, therefore, we'll ommit the indexes 0, 3, 6, 9, ... 3*n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the data to get rid of indexes with unwanted information.\n",
    "unwanted = data[0::3]\n",
    "data = [x for x in data if x not in unwanted]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we're now left only with the sequences, and the labels, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CACACCACACCCACACACCCACACACCACACCACACACCACACCACACCCACACACACACATCCTAACACTACCCTAACACAGCCCTAATCTAACCCTGGCCAACCTGTCTCTCAACTTACCCTCCATTACCCTGCCTCCACTCGTTACCCTGTCCCATTCAACCATACCACTCCGAACCACCATCCATCCCTCTACTTACTACCACTCACCCACCGTTACCCTCCAATTACCCATATCCAACCCACTGCCACTTACCCTACCATTACCCTACCATCCACCATGACCTACTCACCATACTGTTCTTCTAC',\n",
       " '0',\n",
       " 'AATTTTTATAGGTCGACCCTTCTGTCGCTTACTGGGTTGATTATCTTGTGCTTTCTTAGTATCTATCACAAAGGAGACAAAATCGTTGATAAAAAGTGCATCAACATTCCCAGCCAGAAAATGCACATCATAAAGACATGTTATTCAAGAGCCACGACCGTCTTCAATTTATCTTTTATAAAAAACCCTTGTTCTACTGACAGGATGGAATAGATATTAAATATACATTTTGCATTTTTTTTTTTTTCTGTATTGAAGATTTGTATATGAAAGATGTTTATACATCAAATGCTTTGAATAAAGCCATCTTAATTTCAATTTCATGCCCTCCTTCACCGTTTTCTGTTGGTCTAGAGGTAGCTTGTTGTGGTCACTAATGAGAACTTAAATAGTTTTCAACTGCTGGTGGTAAATCAATAATTTATGTTCTTAACCTAACATTTGATGACCTTTGATGCGTTGGTTATGTTGAAGACAAATTGCCTCTAATCAGTTCCATT',\n",
       " '0',\n",
       " 'AATTATATTTCCATCAGCTCAATACCGCAGTACTTTGAAACCTGATTTATATATTGCAGAACTTAATTAAAAGTACATTGTAGTTCAAAAAATAAATATCAAACTTTTGGACCCTCTCTTATTGCCTCCCAATTAATTAAAACATCTTTTCTTCCAATCTACAGGTTTGAAAAGGTAATAAGTAATATAAACTTGAGAACCAAAAAAAAAAAAAAAAAAATACTGATCCTTACAGGTTTTAAGGTTGCAAAGGGAACATTTATTGAAAGGAGCTAACAATAGTGGGTATGAGTAAAGATATATAGATCGATATTTTGAATTCTAAATGATGAACTAGGGAAGTAATTTAGGTGAAACATTGCAACCAATCATTTTACACTTTTGGTTGCACGTAATGTACCTTTTTATGATATTTTTTTTTTATAGTAGTAGTGTGAAAATTTCTTCAGGACTTGCAAAAAGAATCTAACTGATCTTCGGATGAGCCTTTATCGATTATT',\n",
       " '0',\n",
       " 'AACAATAGTGGGTATGAGTAAAGATATATAGATCGATATTTTGAATTCTAAATGATGAACTAGGGAAGTAATTTAGGTGAAACATTGCAACCAATCATTTTACACTTTTGGTTGCACGTAATGTACCTTTTTATGATATTTTTTTTTTATAGTAGTAGTGTGAAAATTTCTTCAGGACTTGCAAAAAGAATCTAACTGATCTTCGGATGAGCCTTTATCGATTATTTTTTTCCTAAATATAATACTTTACAAGCGAATGTTTTGTTAGGAGAAAGATATAAAAATTATGCGGCATAGGCATATTATCCAATAAAAAGGAAATTTATATATAAACTTCATTTACGTCATAAGAAAATGTTAAGTTCTCTTAACGAAAACTGTGCGAATTTTGTGTTAAAGCTGGATGATGAGAAATTATTCTCGTATTATTTTTCATCAGATACTGATAAGGTTTCAACGTCTTTTGACGTTGGCTTTTCCACACCATGTTTAGAGTTATA',\n",
       " '0',\n",
       " 'CAAAGATTTCAACCATAGTAGGTATGCCCCAGATGAAACATTACTGTCATTCCAACAGAAAATATTCCATAAAGTTAAAATGTATATTTTCCAATCACTGAAAATTGTTTTCCACAGTTTAATATCGAATACTTTTGTTTCAAAATCACTTTTACCTGTTTGGTTTTCTTTTAATCTTTTCCTCGCCAACCTAATTTCATCATCAGTTAAGAAAATAGAATAACAGTTGTATGGGTCACCTGGCAGGGAGTAAAATCCAATAAGGCCCACTACGACAGACACAATAGCGTCAATAATAAAGTTCCATCTCCATCCCTCTAAACCATTTACACCATTTAACGATGAATATACGGCTGACTGGATCCCACCAGCGGATAGAATACCGATATACTGGCCCAAATAGTAAAAAGCAGAACGACGCACCATTTCATCATGTTTGTAAAAGGAACCAAACAAATATTGGTATGCCAAATAACTTGGCGCTTCAAAAGCCCCAATGA',\n",
       " '1']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, the next thing would be, that we split this into sequences, and labels. Also, for the output, we'll need to append a 0 if the output is already 1, and vice versa, so that our target variable looks like [0, 1], [1, 0], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = data[0::2]\n",
    "outputs = data[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CACACCACACCCACACACCCACACACCACACCACACACCACACCACACCCACACACACACATCCTAACACTACCCTAACACAGCCCTAATCTAACCCTGGCCAACCTGTCTCTCAACTTACCCTCCATTACCCTGCCTCCACTCGTTACCCTGTCCCATTCAACCATACCACTCCGAACCACCATCCATCCCTCTACTTACTACCACTCACCCACCGTTACCCTCCAATTACCCATATCCAACCCACTGCCACTTACCCTACCATTACCCTACCATCCACCATGACCTACTCACCATACTGTTCTTCTAC',\n",
       " 'AATTTTTATAGGTCGACCCTTCTGTCGCTTACTGGGTTGATTATCTTGTGCTTTCTTAGTATCTATCACAAAGGAGACAAAATCGTTGATAAAAAGTGCATCAACATTCCCAGCCAGAAAATGCACATCATAAAGACATGTTATTCAAGAGCCACGACCGTCTTCAATTTATCTTTTATAAAAAACCCTTGTTCTACTGACAGGATGGAATAGATATTAAATATACATTTTGCATTTTTTTTTTTTTCTGTATTGAAGATTTGTATATGAAAGATGTTTATACATCAAATGCTTTGAATAAAGCCATCTTAATTTCAATTTCATGCCCTCCTTCACCGTTTTCTGTTGGTCTAGAGGTAGCTTGTTGTGGTCACTAATGAGAACTTAAATAGTTTTCAACTGCTGGTGGTAAATCAATAATTTATGTTCTTAACCTAACATTTGATGACCTTTGATGCGTTGGTTATGTTGAAGACAAATTGCCTCTAATCAGTTCCATT',\n",
       " 'AATTATATTTCCATCAGCTCAATACCGCAGTACTTTGAAACCTGATTTATATATTGCAGAACTTAATTAAAAGTACATTGTAGTTCAAAAAATAAATATCAAACTTTTGGACCCTCTCTTATTGCCTCCCAATTAATTAAAACATCTTTTCTTCCAATCTACAGGTTTGAAAAGGTAATAAGTAATATAAACTTGAGAACCAAAAAAAAAAAAAAAAAAATACTGATCCTTACAGGTTTTAAGGTTGCAAAGGGAACATTTATTGAAAGGAGCTAACAATAGTGGGTATGAGTAAAGATATATAGATCGATATTTTGAATTCTAAATGATGAACTAGGGAAGTAATTTAGGTGAAACATTGCAACCAATCATTTTACACTTTTGGTTGCACGTAATGTACCTTTTTATGATATTTTTTTTTTATAGTAGTAGTGTGAAAATTTCTTCAGGACTTGCAAAAAGAATCTAACTGATCTTCGGATGAGCCTTTATCGATTATT',\n",
       " 'AACAATAGTGGGTATGAGTAAAGATATATAGATCGATATTTTGAATTCTAAATGATGAACTAGGGAAGTAATTTAGGTGAAACATTGCAACCAATCATTTTACACTTTTGGTTGCACGTAATGTACCTTTTTATGATATTTTTTTTTTATAGTAGTAGTGTGAAAATTTCTTCAGGACTTGCAAAAAGAATCTAACTGATCTTCGGATGAGCCTTTATCGATTATTTTTTTCCTAAATATAATACTTTACAAGCGAATGTTTTGTTAGGAGAAAGATATAAAAATTATGCGGCATAGGCATATTATCCAATAAAAAGGAAATTTATATATAAACTTCATTTACGTCATAAGAAAATGTTAAGTTCTCTTAACGAAAACTGTGCGAATTTTGTGTTAAAGCTGGATGATGAGAAATTATTCTCGTATTATTTTTCATCAGATACTGATAAGGTTTCAACGTCTTTTGACGTTGGCTTTTCCACACCATGTTTAGAGTTATA',\n",
       " 'CAAAGATTTCAACCATAGTAGGTATGCCCCAGATGAAACATTACTGTCATTCCAACAGAAAATATTCCATAAAGTTAAAATGTATATTTTCCAATCACTGAAAATTGTTTTCCACAGTTTAATATCGAATACTTTTGTTTCAAAATCACTTTTACCTGTTTGGTTTTCTTTTAATCTTTTCCTCGCCAACCTAATTTCATCATCAGTTAAGAAAATAGAATAACAGTTGTATGGGTCACCTGGCAGGGAGTAAAATCCAATAAGGCCCACTACGACAGACACAATAGCGTCAATAATAAAGTTCCATCTCCATCCCTCTAAACCATTTACACCATTTAACGATGAATATACGGCTGACTGGATCCCACCAGCGGATAGAATACCGATATACTGGCCCAAATAGTAAAAAGCAGAACGACGCACCATTTCATCATGTTTGTAAAAGGAACCAAACAAATATTGGTATGCCAAATAACTTGGCGCTTCAAAAGCCCCAATGA']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0', '0', '0', '1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to append either 0, or 1 at the end of an existing 1 or 0 label\n",
    "def append_output(label):\n",
    "    if label == '0':\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(map(append_output, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [0, 1], [0, 1], [0, 1], [1, 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we pretty much have the data converted in the form, from where we can get started with one-hot-encoding of the sequences.\n",
    "For that purpose, we'll be needing the maximum length of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = max(list(map(len, sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxSeqLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all the sequences like \"ATCCGT\" into lists, like [A, T, C, C, G, T]\n",
    "sequences = list(map(list, [[x for x in y] for y in sequences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor(maxSeqLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary processor maps every word (character, in our case) to an ID.\n",
    "seq_proc = np.array(list(vocab_processor.fit_transform(list(map(\" \".join, sequences)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2,\n",
       "       1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 1,\n",
       "       2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 3, 1, 1, 3, 2, 2, 1, 2,\n",
       "       1, 3, 2, 1, 1, 1, 3, 2, 2, 1, 2, 1, 2, 4, 1, 1, 1, 3, 2, 2, 3, 1, 3,\n",
       "       2, 2, 1, 1, 1, 3, 4, 4, 1, 1, 2, 2, 1, 1, 3, 4, 3, 1, 3, 1, 3, 1, 2,\n",
       "       2, 1, 3, 3, 2, 1, 1, 1, 3, 1, 1, 2, 3, 3, 2, 1, 1, 1, 3, 4, 1, 1, 3,\n",
       "       1, 1, 2, 1, 3, 1, 4, 3, 3, 2, 1, 1, 1, 3, 4, 3, 1, 1, 1, 2, 3, 3, 1,\n",
       "       2, 2, 1, 1, 2, 3, 2, 1, 1, 2, 1, 3, 1, 1, 4, 2, 2, 1, 1, 2, 1, 1, 2,\n",
       "       3, 1, 1, 2, 3, 1, 1, 1, 3, 1, 3, 2, 1, 3, 3, 2, 1, 3, 2, 1, 1, 2, 1,\n",
       "       3, 1, 2, 1, 1, 1, 2, 1, 1, 4, 3, 3, 2, 1, 1, 1, 3, 1, 1, 2, 2, 3, 3,\n",
       "       2, 1, 1, 1, 2, 3, 2, 3, 1, 1, 2, 2, 1, 1, 1, 2, 1, 3, 4, 1, 1, 2, 1,\n",
       "       3, 3, 2, 1, 1, 1, 3, 2, 1, 1, 2, 3, 3, 2, 1, 1, 1, 3, 2, 1, 1, 2, 3,\n",
       "       1, 1, 2, 1, 1, 2, 3, 4, 2, 1, 1, 3, 2, 1, 3, 1, 2, 1, 1, 2, 3, 2, 1,\n",
       "       3, 4, 3, 3, 1, 3, 3, 1, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_proc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above few lines mean, that the characters are mapped into IDs in the following way:\n",
    "\n",
    "C -> 1\n",
    "\n",
    "A -> 2\n",
    "\n",
    "T -> 3\n",
    "\n",
    "G -> 4\n",
    "\n",
    "As for the zeroes, the length of the first sequence is smaller than 500, therefore, zeroes are padded. In case any sequence of length greater than 500 (which is impossible in our case because we've adjusted the vocabProcessor according to maxLength), then that sequence gets trimmed to get adjusted into a 500-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "vocab_size = len(vocab_processor.vocabulary_)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicates the vocabulary set {NULL, C, A, T, G}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 1, ..., 0, 0, 0],\n",
       "       [2, 2, 3, ..., 2, 3, 3],\n",
       "       [2, 2, 3, ..., 2, 3, 3],\n",
       "       ..., \n",
       "       [2, 3, 3, ..., 2, 4, 2],\n",
       "       [3, 2, 3, ..., 3, 2, 3],\n",
       "       [3, 3, 4, ..., 4, 2, 2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's the time to integrate all the above steps into a single function, which takes the file path as input, and returns the preprocessed sequence array, and one output label array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_squence(filename):\n",
    "    \"\"\"\n",
    "    Inputs: A directory name\n",
    "    Outputs: A numpy array of one hot encoded features, and a numpy array of appended labels, maxSeqLength, and vocab_size\n",
    "    \"\"\"\n",
    "    #print(\"Files found in the directory: {}\".format(dirList))\n",
    "    data = np.genfromtxt('histone/'+filename, dtype='str')\n",
    "    # Slicing the data to get rid of indexes with unwanted information.\n",
    "    #print(data.shape)\n",
    "    unwanted = data[0::3]\n",
    "    data = [x for x in data if x not in unwanted]\n",
    "    sequences = data[0::2]\n",
    "    outputs = data[1::2]\n",
    "    \n",
    "    # function to append either 0, or 1 at the end of an existing 1 or 0 label\n",
    "    def append_output(label):\n",
    "        if label == '0':\n",
    "            return [0, 1]\n",
    "        else:\n",
    "            return [1, 0]\n",
    "        \n",
    "    labels = list(map(append_output, outputs))\n",
    "    \n",
    "    maxSeqLength = max(list(map(len, sequences)))\n",
    "    #Converting all the sequences like \"ATCCGT\" into lists, like [A, T, C, C, G, T]\n",
    "    sequences = list(map(list, [[x for x in y] for y in sequences]))\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(maxSeqLength)\n",
    "    \n",
    "    # Vocabulary processor maps every word (character, in our case) to an ID.\n",
    "    seq_proc = np.array(list(vocab_processor.fit_transform(list(map(\" \".join, sequences)))))\n",
    "    #applying oneHotEncoding\n",
    "    oneHotFeatures = list(map(to_categorical, seq_proc))\n",
    "    \n",
    "    return np.array(oneHotFeatures), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valid file names are:\n",
    "import os\n",
    "filelist = os.listdir('histone')\n",
    "filelist = ['H4.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_squence(filename='H3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squentially reading rest of the files, and stacking them up to get one single big dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in filelist:\n",
    "#     print(\"reading file {}\".format(f))\n",
    "#     Xtemp, ytemp = preprocess_squence(filename=f)\n",
    "#     X = np.vstack((X, Xtemp))\n",
    "#     y = np.vstack((y, ytemp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  1.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.,  0.]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14965, 500, 5), (14965, 2))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape((X.shape[0], -1, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.]],\n",
       "\n",
       "        [[ 1.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 1.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 1.]],\n",
       "\n",
       "        [[ 0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 1.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 1.]],\n",
       "\n",
       "        [[ 0.]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 1.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 1.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 1.]],\n",
       "\n",
       "        [[ 0.]]],\n",
       "\n",
       "\n",
       "       [[[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        ..., \n",
       "        [[ 1.]],\n",
       "\n",
       "        [[ 0.]],\n",
       "\n",
       "        [[ 0.]]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14965, 2500, 1, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explanation\n",
    "In this project, we propose deep learning model with highly connected blocks contains of two convolutional layers and two max pooling layers as hidden layers. Each convolutional layer is followed by one max pooling layer. Convolutional layers are the main core of CNN. It takes the sequence input as 2-dimensional matrix after applying one-hot vector. Then detect the lower level of features until reach high level features. The input layer is  matrix, where L the length of DNA sequence. The initial number of neurons and weights value are randomly selected.\n",
    "The 2-D convolutions between the kernel vectors wl, of size 2n+1, and the input signal x calculated by the convolutional layers.\n",
    "\n",
    "The output of convolutional layer is rectified linear units (ReLU).\n",
    "\n",
    "In max pooling layer the input is partitions to small regions, then the maximum value picked as output. This layer reduce the complexity of following layers. Max pooling takes filter with size and a stride 2x2 as we noticed in the literature that size is normally used.\n",
    "\n",
    "RNNs such as perceptron and LSTM are a connection among building blocks that designed to exploit sequential information of input data. The main characteristics of RNN layer that capability of handling sequence information over time.\n",
    "\n",
    "The main goal of activation function is introduce nonlinearity that not exit in convolutional layers. Finally, the predicted output obtained through the softmax layer. Here, we chose ReLU as activation function that applies The function: \n",
    "f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to try out these with different configurations\n",
    "from keras.optimizers import rmsprop, adam, SGD, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected input data shape: (batch_size, maxSeqLength, data_dim)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, input_shape=(2500, 1, 1), activation='relu', kernel_size=1, strides=1))\n",
    "model.add(MaxPool2D(pool_size=1, strides=1))\n",
    "model.add(Conv2D(32, activation='relu', kernel_size=1, strides=1))\n",
    "model.add(MaxPool2D(pool_size=1, strides=1))\n",
    "model.add(TimeDistributed(LSTM(16, dropout=0.5, activation='relu', stateful=False)))\n",
    "model.add(Flatten())\n",
    "#model.add(LSTM(8, dropout=0.5, activation='relu', stateful=False))\n",
    "#model.add(LSTM(4, dropout=0.5, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=rmsprop(lr=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 2500, 1, 64)       128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2500, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2500, 1, 32)       2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2500, 1, 32)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 2500, 16)          3136      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 80002     \n",
      "=================================================================\n",
      "Total params: 85,346\n",
      "Trainable params: 85,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative file reading and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X, y lengths: 14965, 14965\n",
      "Split: 13468\n",
      "Shapes X, y: (14965, 2500, 1, 1) (14965, 2)\n",
      "Lengths train X, y: 13468 13468\n",
      "Lengths test X, y: 1497 1497\n"
     ]
    }
   ],
   "source": [
    "# #####\n",
    "# # Temporary Stuff\n",
    "# #model = Sequential()\n",
    "# model.load_weights(filepath='cnn_H3K79me3.txt_weights.h5')\n",
    "# #####\n",
    "print(\"X, y lengths: {}, {}\".format(len(X), len(y)))\n",
    "split = int(len(X)*0.9)\n",
    "print(\"Split: {}\".format(split))\n",
    "print(\"Shapes X, y: {} {}\".format(X.shape, y.shape))\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "\n",
    "print(\"Lengths train X, y: {} {}\".format(len(X_train), len(y_train)))\n",
    "\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]\n",
    "del X, y # to save some space\n",
    "print(\"Lengths test X, y: {} {}\".format(len(X_test), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13468 samples, validate on 1497 samples\n",
      "Epoch 1/20\n",
      "13468/13468 [==============================] - 137s 10ms/step - loss: 0.5554 - acc: 0.7349 - val_loss: 0.5959 - val_acc: 0.6546\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65464, saving model to weights-improvement-01-0.65.hdf5\n",
      "Epoch 2/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.5193 - acc: 0.7618 - val_loss: 0.5904 - val_acc: 0.6754\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65464 to 0.67535, saving model to weights-improvement-02-0.68.hdf5\n",
      "Epoch 3/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.5072 - acc: 0.7717 - val_loss: 0.5919 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67535 to 0.68470, saving model to weights-improvement-03-0.68.hdf5\n",
      "Epoch 4/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.5007 - acc: 0.7787 - val_loss: 0.5936 - val_acc: 0.7281\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.68470 to 0.72812, saving model to weights-improvement-04-0.73.hdf5\n",
      "Epoch 5/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4960 - acc: 0.7776 - val_loss: 0.5831 - val_acc: 0.7114\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4929 - acc: 0.7823 - val_loss: 0.5915 - val_acc: 0.7174\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.4929 - acc: 0.7822 - val_loss: 0.5962 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.4901 - acc: 0.7801 - val_loss: 0.5899 - val_acc: 0.7074\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4886 - acc: 0.7823 - val_loss: 0.5894 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4854 - acc: 0.7840 - val_loss: 0.6058 - val_acc: 0.7241\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4851 - acc: 0.7874 - val_loss: 0.5928 - val_acc: 0.7114\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4836 - acc: 0.7850 - val_loss: 0.5926 - val_acc: 0.7014\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4822 - acc: 0.7889 - val_loss: 0.5965 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.4802 - acc: 0.7863 - val_loss: 0.6159 - val_acc: 0.7148\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4822 - acc: 0.7871 - val_loss: 0.6013 - val_acc: 0.7007\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4792 - acc: 0.7871 - val_loss: 0.5944 - val_acc: 0.6874\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.4806 - acc: 0.7869 - val_loss: 0.5952 - val_acc: 0.7088\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.4792 - acc: 0.7914 - val_loss: 0.5990 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/20\n",
      "13468/13468 [==============================] - 138s 10ms/step - loss: 0.4770 - acc: 0.7905 - val_loss: 0.5986 - val_acc: 0.7007\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/20\n",
      "13468/13468 [==============================] - 139s 10ms/step - loss: 0.4775 - acc: 0.7899 - val_loss: 0.6040 - val_acc: 0.6860\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "#model.load_weights('weights-improvement-07-0.67.hdf5')\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train,\n",
    "      batch_size=64, epochs=20,\n",
    "      validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "#saving the weights of the model\n",
    "model.save_weights(filepath='cnn_rnn_dna_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
